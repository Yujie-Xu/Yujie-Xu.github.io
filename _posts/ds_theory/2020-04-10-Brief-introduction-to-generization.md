---
title: Generization Error
date: 2020-04-11 -0400
categories: [machine-learning, theory]
tag: [machine-learning]
mathjax:  True
---

{% include mathjax.html %}

   In data science, we want to pick an accurate model. There are two fundamental questions:

* How well the model <span style="color: red"> approximates </span> the training data;
* How well the model can be <span style="color: red">generated </span> from training data to the unseen data.



 In general, we have


 $$E_{\text{out}}(g) \lt  E_{\text{in}}(g) + \Omega, , \tag{1}\label{eq:one}$$




where $g$ is the chosen model (hypothesis),  $E_{\text{out}}(g)$ is __out-of-sample error__, $E_{\text{in}}(g)$ is __in-sample error__, and $\Omega$ is the __generization error__.

Our goal is to minimze $E_{\text{out}}(g)$. $E_{\text{out}}(g)$ is used to measure how well this model performs in the whole data. In order to minimze $E_{\text{out}}(g)$, we need to minimze $E_{\text{in}}(g)$ and $\Omega$ based on equation (1). $E_{\text{in}}(g)$ meaures how well the model  approximates  the training data, and $\Omega$ measures how well the model can be generated  from training data to the unseen data.


Intuitively, we want:
* The chosen model performs well in training data, i.e., choose a proper model $g$ to **minimze** $E_{\text{in}}(g)$;
* The performance of the model on training data is close to that of the whole data, i.e, choose a proper model $g$ (more specifially, the hypothesis set) to **minimize** $\Omega$.


In this article, we focuse how to minimze the __generization error__ $\Omega$.


<h2>Generization Error from Hoeffding's Inequaltiy</h2>

<h3> Main Results </h3>

With probability $1-\delta$, we have

$$
E_{\text{out}}(g) < E_{\text{in}}(g) +
\sqrt{\frac{1}{2N}\ln\frac{2M}{\delta}},
$$
where $N$ is the number of training points, and $M$ is the number of hypothesis in the hypothesis sets $\mathcal{H}$. Note that $g$ belongs to $\mathcal{H}$.


Hence, the generization error in this case is

$$
\Omega = \sqrt{\frac{1}{2N}\ln\frac{2M}{\delta}}.
$$

<h3> Analysis </h3>

* This bound is based on the union bound, which is loose.
* The generization error is monotonically increasing w.r.t the number of hypothesis in the hypothesis set $M$. Hence, the hypothesis set with more hypothesis result in larger generization error.
* The generization error is monotonically decreasing w.r.t the number of training points $N$. Hence, we can reduce the generization error by increasing the number of training points.

<h3> Drawbacks </h3>

Ideally, we want

$$
\Omega = \sqrt{\frac{1}{2N}\ln\frac{2M}{\delta}} \rightarrow 0.
$$

If $\sqrt{\frac{\ln M}{N}} \rightarrow 0$, then $\Omega \rightarrow 0$. If $M$ is finite, then we can simply increase the number of training points, $N$. However, in most cases, we deal with a __infinite hypothesis set__, where $M = \infty$. In these cases, the generation error from Hoeffing's inequaility will not be meaning since we can not guarantee the generization erro $\Omega$ approaches $0$.

<h2>Generization Error from VC Dimension</h2>

<h3> Some Concepts </h3>

 <h5> Dichotomy  </h5>

Given a hypothesis $h: \mathcal{X} \rightarrow \{+1, -1\}$, a <strong>dichotomy</strong> is defined as an N-tuple
$$(h(\mathbf{x}_1),...,h(\mathbf{x}_N)),$$

for points $\mathbf{x}_1$, ..., $\mathbf{x}_1$ in input space $\mathcal{X}$. The tuple $(h(\mathbf{x}_1),...,h(\mathbf{x}_N))$ can is a <strong>sequence pattern</strong>.

 Given a hypotheis <em>set</em> $\mathcal{H}$, where  $h: \mathcal{X} \rightarrow \{+1, -1\}$ for $h \in \mathcal{H}$, the <strong> dichotomies generated by $\mathcal{H}$ </strong> are defined as
 $$
 \mathcal{H}(\mathbf{x}_1, ..., \mathbf{x}_N) =
 \{(h(\mathbf{x}_1),...,h(\mathbf{x}_N))|h \in \mathcal{H}\}.
 $$
  where $\mathbf{x}_1$, ..., $\mathbf{x}_N$ are in input space $\mathcal{X}$.




The concept of dichotomy measures the <strong> effective number</strong> in a hypothesis set. Given points $\mathbf{x}_1$, ..., $\mathbf{x}_1$ in $\mathcal{X}$, and hypothesises $h_1$ and $h_2$
in $\mathcal{H}$, if
$$(h_1(\mathbf{x}_1),...,h_1(\mathbf{x}_N))
 =(h_2(\mathbf{x}_1),...,h_2(\mathbf{x}_N)),$$
then $h_1$ and $h_2$ are equivalent in  $\mathcal{H}(\mathbf{x}_1, ..., \mathbf{x}_N)$.


Note that $\mathcal{H}(\mathbf{x}_1, ..., \mathbf{x}_N)$ depends on $\mathbf{x}_1$, ..., $\mathbf{x}_N$. The next step is to get rid of this dependency.


 <h5> Growth Function and VC dimension  </h5>

 In order to get the maximum number of effeive hypothesis for all cases, we need to go through all $N$ and $\mathbf{x}_1$, ..., $\mathbf{x}_N$, and pick the maximum.


 The first  step is to get rid of $\mathbf{x}_1, ..., \mathbf{x}_N$.

 Given $N$ points, we define the <strong>growth function</strong> $m_{\mathcal{H}}(N)$ as
$$
m_{\mathcal{H}}(N) = \max_{\mathbf{x}_1, ..., \mathbf{x}_N \in \mathcal{X}} |\mathcal{H}(\mathbf{x}_1, ..., \mathbf{x}_N)|.
$$

Growth function $m_{\mathcal{H}}(N)$ characterizes the maximum number of possible effective hypothesis (different sequences) of a hypothesis set $\mathcal{H}$ and the number of points $N$.



The second step is to get rid of $N$. It is not easy to get rid of $N$ completely. It is  likely that when $N$ goes to infinit, there are infinite number of different possible sequences $(h(\mathbf{x}_1),...,h(\mathbf{x}_N))$.


Hence, the easier way is to bound \\(m_{\mathcal{H}}(N)\\). It is easy to see that $m_{\mathcal{H}}(N) \leq 2^N$. A straight-forward question is when this inequality becomes strict. We define the <strong>VC dimension $d_{vc}$</strong> as the largest N satisfies that
$$
m_{\mathcal{H}}(N) = 2^N \quad \text{and}
\quad m_{\mathcal{H}}(N+1) < 2^{N+1}.
$$

One can show that
$$
m_{\mathcal{H}}(N) < N^{d_{vc}}+1.
$$

The significance of the VC dimension is that the growth function (number of hypothesises given $N$) grows in a <strong>polynomial</strong> manner instead of exponential manner, given $d_{vc} < \infty$. This reduces the number of hypothesises $2^N$ to the number of effective hypothesises $N^{d_{vc}}+1$.


<strong>The VC dimension $d_{vc}$ may still be finite even when the number of hypothesises in the hypothesis set $\mathcal{H}$ is infinite.</strong>



<h3> Main Results-VC Generation Bound </h3>

With probability $1-\delta$, we have

$$
E_{\text{out}}(g) < E_{\text{in}}(g) +
\sqrt{\frac{8}{N}\ln\frac{4m_{\mathcal{H}}(N)}{\delta}},
$$
where $N$ is the number of training points, and  $\mathcal{H}$ is the hypothesis set.



<h3> Analysis </h3>

* Compare this VC bound to the previous one, it is easy to see this bound uses growth function $m_{\mathcal{H}}(N)$ (the number of effective hypothesis) instead of the number of hypothesis $M$.

* When the VC dimension $d_{vc}$ is finite, we have the generization error

$$
\lim_{N \rightarrow \infty} \sqrt{\frac{8}{N}\ln\frac{4m_{\mathcal{H}}(N)}{\delta}} \leq \lim_{N \rightarrow \infty} \sqrt{\frac{8}{N}\ln\frac{4(N^{d_{vc}}+1)}{\delta}}  = 0.
$$

It shows that the generization error is close to 0 when the number of training points is sufficiently large.

* The speed of the generization error approaching 0 is realted to $d_{vc}$. Generally speaking, more complex hypothesis has larger $d_{vc}$. Hence, in order to achieve similar generization error, the complex hypothesis requires more training points.
